{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接代理相关包\n",
    "# pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2020 World Series was played at Globe Life Field in Arlington, Texas.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:15732\"                # 指定代理，解决连接问题\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:15732\"\n",
    "\n",
    "# 加载 .env 文件中的变量\n",
    "load_dotenv()\n",
    "api_key = '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'\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:15732\"                # 指定代理，解决连接问题\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:15732\"\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm.invoke(\"how can langsmith help with testing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 智能笔记本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "# pip install PyQt6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件夹选择\n",
    "from PyQt5 import QtCore, QtWidgets\n",
    "import sys\n",
    "##########################################\n",
    "#ui界面设置\n",
    "class Ui_MainWindow(object):\n",
    "\n",
    "    def setupUi(self, MainWindow):\n",
    "    \n",
    "        #主窗口参数设置\n",
    "        MainWindow.setObjectName(\"MainWindow\")\n",
    "        MainWindow.resize(848, 721)\n",
    "        self.centralwidget = QtWidgets.QWidget(MainWindow)\n",
    "        self.centralwidget.setObjectName(\"centralwidget\")\n",
    "\n",
    "        # 设置按键参数\n",
    "        self.file = QtWidgets.QPushButton(self.centralwidget)\n",
    "        self.file.setGeometry(QtCore.QRect(57, 660, 175, 28))\n",
    "        self.file.setObjectName(\"file\")\n",
    "        self.file.setStyleSheet(\"background-color:rgb(111,180,219)\")\n",
    "        self.file.setStyleSheet(\n",
    "            \"QPushButton{background-color:rgb(111,180,219)}\"  # 按键背景色\n",
    "            \"QPushButton:hover{color:green}\"  # 光标移动到上面后的前景色\n",
    "            \"QPushButton{border-radius:6px}\"  # 圆角半径\n",
    "            \"QPushButton:pressed{background-color:rgb(180,180,180);border: None;}\"  # 按下时的样式\n",
    "        )\n",
    "\n",
    "        # 设置显示窗口参数\n",
    "        self.fileT = QtWidgets.QPushButton(self.centralwidget)\n",
    "        self.fileT.setGeometry(QtCore.QRect(300, 660, 480, 28))\n",
    "        self.fileT.setObjectName(\"file\")\n",
    "        self.fileT.setStyleSheet(\"background-color:rgb(111,180,219)\")\n",
    "        self.fileT.setStyleSheet(\n",
    "            \"QPushButton{background-color:rgb(111,180,219)}\"  # 按键背景色\n",
    "            \"QPushButton:hover{color:green}\"  # 光标移动到上面后的前景色\n",
    "            \"QPushButton{border-radius:6px}\"  # 圆角半径\n",
    "            \"QPushButton:pressed{background-color:rgb(180,180,180);border: None;}\"  # 按下时的样式\n",
    "        )\n",
    "\n",
    "\n",
    "        #主窗口及菜单栏标题栏设置\n",
    "        MainWindow.setCentralWidget(self.centralwidget)\n",
    "        self.menubar = QtWidgets.QMenuBar(MainWindow)\n",
    "        self.menubar.setGeometry(QtCore.QRect(0, 0, 848, 26))\n",
    "        self.menubar.setObjectName(\"menubar\")\n",
    "        MainWindow.setMenuBar(self.menubar)\n",
    "        self.statusbar = QtWidgets.QStatusBar(MainWindow)\n",
    "        self.statusbar.setObjectName(\"statusbar\")\n",
    "        MainWindow.setStatusBar(self.statusbar)\n",
    "\n",
    "        self.retranslateUi(MainWindow)\n",
    "        QtCore.QMetaObject.connectSlotsByName(MainWindow)\n",
    "\n",
    "        ################button按钮点击事件回调函数################\n",
    "\n",
    "        self.file.clicked.connect(self.msg)\n",
    "\n",
    "\n",
    "    def retranslateUi(self, MainWindow):\n",
    "        _translate = QtCore.QCoreApplication.translate\n",
    "        MainWindow.setWindowTitle(_translate(\"MainWindow\", \"Deecamp_Eurus\"))\n",
    "        self.file.setText(_translate(\"MainWindow\", \"选择文件\"))\n",
    "        self.fileT.setText(_translate(\"MainWindow\", \"\"))\n",
    "\n",
    "    #########选择图片文件夹#########\n",
    "\n",
    "    def msg(self,Filepath):\n",
    "        m = QtWidgets.QFileDialog.getExistingDirectory(None,\"选取文件夹\",\"C:/\")  # 起始路径\n",
    "        self.fileT.setText(m)\n",
    "\n",
    "#########主函数入口 #########\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app = QtWidgets.QApplication(sys.argv)\n",
    "\n",
    "    mainWindow = QtWidgets.QMainWindow()\n",
    "\n",
    "    ui = Ui_MainWindow()\n",
    "\n",
    "    ui.setupUi(mainWindow)\n",
    "\n",
    "    mainWindow.show()\n",
    "\n",
    "    sys.exit(app.exec_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 连接数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mysql.connector\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 连接mysql，进行增删改查操作\n",
    "class MySQLHandler:\n",
    "    def __init__(self):\n",
    "        self.config = {\n",
    "            'user': 'root',\n",
    "            'password': '',\n",
    "            'host': 'localhost' ,\n",
    "            'port': '3306' ,\n",
    "            'database': 'db',\n",
    "            'raise_on_warnings': True,\n",
    "        }\n",
    "        self.conn = None\n",
    "        self.cursor = None\n",
    "\n",
    "    def connect_to_mysql(self):\n",
    "        try:\n",
    "            self.conn = mysql.connector.connect(**self.config)\n",
    "            self.cursor = self.conn.cursor(buffered=True, dictionary=True)\n",
    "        except mysql.connector.Error as err:\n",
    "            logger.error(\"Error connecting to MySQL: %s\", err)\n",
    "\n",
    "    def execute_query(\n",
    "        self,\n",
    "        query: str,\n",
    "        data=(),\n",
    "        single: bool = False\n",
    "    ):\n",
    "        try:\n",
    "            self.connect_to_mysql()\n",
    "            self.cursor.execute(query, data)\n",
    "\n",
    "            if (single):\n",
    "                return self.cursor.fetchone()\n",
    "            return self.cursor.fetchall()\n",
    "\n",
    "        except mysql.connector.Error as err:\n",
    "            print(\"Error: {}\".format(err))\n",
    "            return None\n",
    "\n",
    "    # update table data\n",
    "    def update_table_data(\n",
    "        self,\n",
    "        query: str,\n",
    "        data=()\n",
    "    ):\n",
    "        try:\n",
    "            self.connect_to_mysql()\n",
    "            self.cursor.execute(query, data)\n",
    "            self.conn.commit()\n",
    "            print(f\"Data updated successfully.\")\n",
    "\n",
    "        except mysql.connector.Error as err:\n",
    "            self.conn.rollback()\n",
    "            print(\"Error: {}\".format(err))\n",
    "\n",
    "        finally:\n",
    "            self.disconnect_from_mysql()\n",
    "\n",
    "    # insert table data\n",
    "    def insert_table_data(\n",
    "        self,\n",
    "        query,\n",
    "        data=()\n",
    "    ):\n",
    "        try:\n",
    "            self.connect_to_mysql()\n",
    "            self.cursor.execute(query, data)\n",
    "            self.conn.commit()\n",
    "            print(\"New data inserted successfully.\")\n",
    "            inserted_id = self.cursor.lastrowid\n",
    "            return inserted_id\n",
    "\n",
    "        except mysql.connector.Error as err:\n",
    "            self.conn.rollback()\n",
    "            print(\"Error: {}\".format(err))\n",
    "\n",
    "        finally:\n",
    "            self.disconnect_from_mysql()\n",
    "\n",
    "    # delete table data by id\n",
    "    def delete_table_data(self, query, data=()):\n",
    "        try:\n",
    "            self.connect_to_mysql()\n",
    "            self.cursor.execute(query, data)\n",
    "            self.conn.commit()\n",
    "            print(\"Data deleted successfully.\")\n",
    "\n",
    "        except mysql.connector.Error as err:\n",
    "            self.conn.rollback()\n",
    "            print(\"Error: {}\".format(err))\n",
    "\n",
    "        finally:\n",
    "            self.disconnect_from_mysql()\n",
    "\n",
    "    # disconnect\n",
    "    def disconnect_from_mysql(self):\n",
    "        try:\n",
    "            if self.cursor is not None:\n",
    "                self.cursor.close()\n",
    "            if self.conn is not None and self.conn.is_connected():\n",
    "                self.conn.close()\n",
    "\n",
    "        except mysql.connector.Error as err:\n",
    "            print(\"Error: {}\".format(err))\n",
    "\n",
    "\n",
    "mysqll = MySQLHandler()\n",
    "mysqll.connect_to_mysql()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 上传文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_file_content(file_path):\n",
    "  \"\"\"\n",
    "  获取指定文件内容\n",
    "\n",
    "  Args:\n",
    "    file_path: 文件路径\n",
    "\n",
    "  Returns:\n",
    "    文件内容 (str)\n",
    "  \"\"\"\n",
    "  if not os.path.exists(file_path):\n",
    "    print(\"file path not exist !!\")\n",
    "    return None\n",
    "\n",
    "  with open(file_path, 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "  return content\n",
    "\n",
    "file_path = \"D:/桌面/examor-main/data/电路分析.md\"\n",
    "\n",
    "doc = get_file_content(file_path)\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='# 电路分析\\n\\n研究对象；电能、信号\\n\\n从能量角度来看；电源：供能；负载：耗能；连接：传输、分配、控制；\\n\\n现实中的元件都是 集总参数元件（实际元件=多个理想元件交织在一起）要有条件地近似——当电路的几何尺寸<<$f_{max}$时对应的$\\\\lambda$时可近似（同一元件在不同$f$的电路中等效的元件可能不同）；\\n\\n##### 电路变量\\n\\n【1】电流——单位时间通过界面的电荷量\\n\\ni=$\\\\frac{dq(电荷量（单位：c）)}{dt(时间)}$\\n\\n【2】电压——单位正电荷在电场中由a到b所用的能量\\nu=$\\\\frac{dw(能量(J))}{dq}$\\n\\n【3】功率\\np=$\\\\frac{dw}{dt}=+/-ui$（若u、i关联（电流与电压是否有因果关系（电流的方向指流经该线路时的方向，而不是流出时的））则为正，反之为负）（正—消耗的功率；负—放出的功率）\\n\\n网孔指不含支路的回路；\\n\\n电流和电压(由高电压到低电压,)均有方向——先设一个参考方向，如果求出来是负数则设反了：\\n\\n基尔霍夫\\n电流定律（KCL）（本质是电荷守恒）：流入任一点的i代数和(流入-流出)为0；\\n电压定律（KVL）（本质是能量守恒）：任一回路的u和(升压-降压)为0；\\n\\n电阻（使u、i发生变化）：u-i图上的一条曲线（为负值时就表示电阻大小，没有其他实际意义）\\n\\nVCR（伏安特性）（时变：VCR曲线随时间变化）\\n$r=\\\\frac{u}{i}$;电导G(为了计算)（单位：西门子(S)=$\\\\frac{A}{V}$）=$\\\\frac{1}{R}$=$\\\\frac{i}{u}$')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 指定代理，解决连接问题\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:15732\"               \n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:15732\"\n",
    "\n",
    "# -> list[Document]\n",
    "\n",
    "# 最大token\n",
    "MAX_TOKEN = 2700\n",
    "\n",
    "# 计算字符串长度\n",
    "def len_token(s: str):\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(enc.encode(s))\n",
    "\n",
    "# 检查字符串中是否有成对的引号\n",
    "def is_odd_backtick_paired(s: str) -> bool:\n",
    "    counts = s.count('```')\n",
    "    if counts == 0:\n",
    "        return False\n",
    "    elif counts % 2 == 0:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 检查是否过短（少于200 token）\n",
    "def is_there_no_enough_content(s: str) -> bool:\n",
    "    return bool(len_token(s) < 200)\n",
    "\n",
    "# 检查是否达到最大token\n",
    "def is_the_token_exceeded(s: str) -> bool:\n",
    "    return bool(len_token(s) > MAX_TOKEN)\n",
    "\n",
    "# 文档分割函数\n",
    "def split_doc(doc_content):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=MAX_TOKEN, # 文本块的最大长度\n",
    "        chunk_overlap=0, # 文本间的重叠长度\n",
    "        is_separator_regex=True, # 指定分隔符为正则表达式\n",
    "        length_function=len_token, # 计算文本长度的函数\n",
    "        separators=[ # 识别匹配规则\n",
    "            # Matches first-level title followed by second, third, or fourth-level title\n",
    "            \"\\n#\\s[^\\n]+\\n\\n?##+?\\s[^\\n]+\",\n",
    "            \"\\n#{1,6}\\s\",\n",
    "            # Matches three or more dashes followed by a newline\n",
    "            \"\\n-{3,}\\n\",\n",
    "            # Matches three or more underscores followed by a newline\n",
    "            \"\\n_{3,}\\n\",\n",
    "            # Matches unordered list items starting with '-'\n",
    "            \"\\n\\s*-\\s[^\\n]+\\n\",\n",
    "            # Matches ordered list items\n",
    "            \"\\n\\s*\\d+[.)]\\s[^\\n]+\\n\",\n",
    "            # Matches open code block\n",
    "            \"\\n```\\w+\\n\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    docs = text_splitter.create_documents([doc_content])\n",
    "    res: list[Document] = []\n",
    "    for doc in docs:\n",
    "        content = doc.page_content\n",
    "        if is_odd_backtick_paired(content):\n",
    "            continue\n",
    "        if is_there_no_enough_content(content):\n",
    "            continue\n",
    "        if is_the_token_exceeded(content):\n",
    "            continue\n",
    "        res.append(doc)\n",
    "\n",
    "    return res\n",
    "\n",
    "doc_list = split_doc(doc)\n",
    "print(doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义大模型参数\n",
    "import os\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI, ChatAnthropic\n",
    "\n",
    "class LLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature: int = 0,\n",
    "        streaming: bool = False,\n",
    "        callbacks: list = [],\n",
    "        max_retries=3,\n",
    "        max_tokens=None,\n",
    "        timeout=10\n",
    "    ):\n",
    "        self.temperature = temperature if temperature != 0 else self._get_role_temperature()\n",
    "        self.streaming = streaming\n",
    "        self.callbacks = callbacks\n",
    "        self.max_retries = max_retries\n",
    "        self.max_tokens = max_tokens\n",
    "        self.timeout = timeout\n",
    "        self.llm = self._init_openai()\n",
    "\n",
    "    def _init_openai(self) -> ChatOpenAI:\n",
    "        return ChatOpenAI(\n",
    "            openai_api_base=os.environ[\"OPENAI_BASE\"]+\"/v1\",\n",
    "            openai_proxy=os.environ['OPENAI_API_PROXY'],\n",
    "            model=os.environ[\"OPENAI_MODEL\"],\n",
    "            temperature=self.temperature,\n",
    "            streaming=self.streaming,\n",
    "            callbacks=self.callbacks,\n",
    "            max_retries=self.max_retries,\n",
    "            max_tokens=self.max_tokens,\n",
    "            request_timeout=self.timeout,\n",
    "        )\n",
    "\n",
    "    def _get_role_temperature(self):\n",
    "        if (os.environ.get(\"CURRENT_ROLE\") == \"examiner\"):\n",
    "            return 0\n",
    "        if (os.environ.get(\"CURRENT_ROLE\") == \"teacher\"):\n",
    "            return 0.5\n",
    "        if (os.environ.get(\"CURRENT_ROLE\") == \"interviewer\"):\n",
    "            return 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成问题\n",
    "\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "examiner = \"\"\"\n",
    "您是一位十分严格的考官，我需要您以这个身份生成问题。\n",
    "我会给您一个标题表示上下文对应的主题，请您严格的根据上下文内容出题，你不可以生成与上下文内容无关的问题\n",
    "\"\"\"\n",
    "\n",
    "teacher = \"\"\"\n",
    "您是一位慈祥且认真的教师，我需要您以这个身份生成问题。\n",
    "我会给您一个标题表示上下文对应的主题，请您根据上下文内容出题，您可以根据您的所知稍微对上下文内容进行扩展发散，但您不可以捏造您不知道的知识。\n",
    "\"\"\"\n",
    "\n",
    "interviewer = \"\"\"\n",
    "您是一位有着多年工作资质的面试官，我需要您以这个身份生成问题。\n",
    "我会给您一个标题表示上下文对应的主题，您可以以上下文内容为基础扩展性的出题。也就是说，问题不必非在上下文中产生，但您不可以捏造您不知道的知识。\n",
    "\"\"\"\n",
    "\n",
    "short = \"\"\"\n",
    "您需要提出尽可能多的问题(最多10个)，您生成的问题要覆盖上下文中的各个知识点，但所有的问题不能有任何重复的内容。\n",
    "\n",
    "问题(以markdown语法，不携带数字的列表):\n",
    "\"\"\"\n",
    "\n",
    "choice = \"\"\"\n",
    "您需要提出尽可能多的单选题（最多7个）并为每道题附带 4 个选项且只能有一个正确答案，你生成的问题要覆盖上下文中的各个知识点，但所有的问题不能有任何重复的内容。\n",
    "\n",
    "请您按照以下格式出题:\n",
    "'''\n",
    "- xxxx:\n",
    "    A. xxxx\n",
    "    B. xxxx\n",
    "    C. xxxx\n",
    "    D. xxxx\n",
    "\n",
    "'''\n",
    "\n",
    "单选题(以markdown语法):\n",
    "\"\"\"\n",
    "\n",
    "blank = \"\"\"\n",
    "您需要提出填空题(最多8个)，您生成的问题要覆盖上下文中的各个知识点，但所有的问题不能有任何重复的内容。不可以将答案写在题目中！\n",
    "\n",
    "请您按照以下格式出题:\n",
    "'''\n",
    "- xxxxxxx______xxxx\n",
    "'''\n",
    "\n",
    "填空题(以markdown语法):\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TEMPLATE = '''\n",
    "### 标题 ###\n",
    "{title}\n",
    "\n",
    "### 上下文 ###\n",
    "{context}\n",
    "'''\n",
    "\n",
    "\n",
    "def _get_role():\n",
    "    current_role = os.environ.get(\"CURRENT_ROLE\")\n",
    "    if current_role == \"teacher\":\n",
    "        return teacher\n",
    "    elif current_role == \"interviewer\":\n",
    "        return interviewer\n",
    "    else:\n",
    "        return examiner\n",
    "\n",
    "\n",
    "def _get_question_type(type):\n",
    "    if type == \"choice\":\n",
    "        return choice\n",
    "    elif type == \"blank\":\n",
    "        return blank\n",
    "    else:\n",
    "        return short\n",
    "\n",
    "\n",
    "def get_question_generate_cn(type):\n",
    "    QUESTION_GENERATE_PROMPT_CN = PromptTemplate(\n",
    "        template=_get_role() + PROMPT_TEMPLATE + _get_question_type(type),\n",
    "        input_variables=[\"title\", \"context\"]\n",
    "    )\n",
    "    return QUESTION_GENERATE_PROMPT_CN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回答反馈\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "examiner = \"\"\"\n",
    "作为严格的考官，您需要基于上下文和问题对我的答案进行十分严格评分（分数范围：0-10）。\n",
    "\"\"\"\n",
    "\n",
    "teacher = \"\"\"\n",
    "作为一位资深教师，您需要根据自己丰富的教学经验基于上下文和问题对我的答案进行评分（分数范围：0-10）。\n",
    "注意！：请以您平时对学生作答的方式进行打分，不必严格，但是要保证给出的分数是站在学生认真的回答了问题的基础上的。\n",
    "注意！：问题可能是基于上下文进行扩展的，您的回答也应以一位端庄友善的老师的口吻来呈现。\n",
    "\"\"\"\n",
    "\n",
    "interviewer = \"\"\"\n",
    "你是一位风趣且有着资深经验的面试官，您需要根据自己丰富的教学经验基于上下文和问题对我的答案进行评分（分数范围：0-10）。\n",
    "注意！：请以您平时对面试者作答的方式进行打分就如同真的处于面试的环境之中，请相对严格，但是要保证给出的分数是站在面试者认真的回答了问题的基础上的。\n",
    "注意！：问题是基于上下文进行扩展的，您也需要适当的依据已知的知识进行扩展，但请不要随意编造答案，您的回答也应以一位风趣并十分专业的面试官的口吻来呈现。\n",
    "\"\"\"\n",
    "\n",
    "short = \"\"\"\n",
    "请您按照以下格式回答:\n",
    "**得分**：x\n",
    "**检测**：\n",
    "xxx\n",
    "**正确答案**：\n",
    "xxx\n",
    "\n",
    "请您对我的答案进行纠错，将您纠错的内容填写在“检测”部分。并且根据上下文，为问题提供一个合适的回答，填写到 ”正确答案“ 部分。\n",
    "您的回答（请使用 markdown 语法）：\n",
    "\"\"\"\n",
    "\n",
    "choice = \"\"\"\n",
    "请您按照以下格式回答:\n",
    "**得分**：x\n",
    "**检测**：\n",
    "xxx\n",
    "**正确答案**：\n",
    "A. xxx\n",
    "\n",
    "请对这道选择题，根据上下文对我的答案进行纠错与打分（分数只有0和10）\n",
    "您的回答（请使用 markdown 语法）：\n",
    "\"\"\"\n",
    "\n",
    "blank = \"\"\"\n",
    "请您按照以下格式回答:\n",
    "**得分**：x\n",
    "**检测**：\n",
    "xxx\n",
    "**正确答案**：\n",
    "xxx\n",
    "\n",
    "请对这道填空题，根据上下文对我的答案进行纠错与打分\n",
    "您的回答（请使用 markdown 语法）：\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TEMP = '''\n",
    "### 上下文：\n",
    "{context}\n",
    "####\n",
    "\n",
    "### 问题：\n",
    "{question}\n",
    "####\n",
    "\n",
    "### 我的答案：\n",
    "{answer}\n",
    "####\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def _get_role_prompt(role: str):\n",
    "    if role == \"teacher\":\n",
    "        return teacher\n",
    "    elif role == \"interviewer\":\n",
    "        return interviewer\n",
    "    else:\n",
    "        return examiner\n",
    "\n",
    "\n",
    "def _get_question_type(type: str):\n",
    "    if type == \"choice\":\n",
    "        return choice\n",
    "    elif type == \"blank\":\n",
    "        return blank\n",
    "    else:\n",
    "        return short\n",
    "\n",
    "\n",
    "def get_examine_prompt_cn(role: str, question_type: str):\n",
    "    ANSWER_EXAMINE_PROMPT_CN = PromptTemplate(\n",
    "        template=_get_role_prompt(role) + PROMPT_TEMP +\n",
    "        _get_question_type(question_type),\n",
    "        input_variables=[\"context\", \"question\", \"answer\"]\n",
    "    )\n",
    "    return ANSWER_EXAMINE_PROMPT_CN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化prompt\n",
    "\n",
    "# from .cn.question_generate import get_question_generate_cn\n",
    "# from .en.question_generate import get_question_generate_en\n",
    "\n",
    "# from .cn.answer_examine import get_examine_prompt_cn\n",
    "# from .en.answer_examine import get_examine_prompt_en\n",
    "\n",
    "# input: 角色、问题类型（选择、填空）、语言（中英）、提示词类型（生成问题、回答反馈）\n",
    "def choose_prompt(\n",
    "    role: str,\n",
    "    question_type: str,\n",
    "    prompt_language: str,\n",
    "    prompt_type: str,\n",
    "):\n",
    "    prompt = None\n",
    "    \n",
    "    # 为了简化，取消掉了英文问题生成和回复，后面只需要加入'examor-main\\server\\prompts\\en'中的代码即可。\n",
    "\n",
    "    # if (prompt_language == \"en\"):\n",
    "    #     if (prompt_type == \"question_generate\"):\n",
    "    #         prompt = get_question_generate_en(question_type)\n",
    "    #     if (prompt_type == \"answer_examine\"):\n",
    "    #         prompt = get_examine_prompt_en(role, question_type)\n",
    "\n",
    "    if (prompt_language == \"zh-CN\"):\n",
    "        if (prompt_type == \"question_generate\"):\n",
    "            prompt = get_question_generate_cn(question_type)\n",
    "        if (prompt_type == \"answer_examine\"):\n",
    "            prompt = get_examine_prompt_cn(role, question_type)\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成提问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import asyncio\n",
    "import datetime\n",
    "from typing import Awaitable\n",
    "from langchain import LLMChain\n",
    "from langchain.schema import Document\n",
    "from langchain.callbacks import AsyncIteratorCallbackHandler\n",
    "\n",
    "import db_services as _dbs_\n",
    "from .langchain_llm import LLM\n",
    "from utils.ebbinghaus import handle_ebbinghaus_memory\n",
    "from prompts import choose_prompt\n",
    "\n",
    "\n",
    "class Chain:\n",
    "\n",
    "    # 初始化所有的参数，包括并发访问量（根据是否付费判断）、笔记id、文件id、文件名、语言、提示类型、随机读、是否启用流式生成、\n",
    "    # LLM 回调句柄列表，用于处理 LLM 生成的文本、生成的总问题数量。\n",
    "    def __init__(\n",
    "        self,\n",
    "        note_id: int = 0,\n",
    "        file_id: int = 0,\n",
    "        filename: str = \"\",\n",
    "        prompt_language: str = \"\",\n",
    "        prompt_type: str = \"\",\n",
    "        temperature: int = 0,\n",
    "        streaming: bool = False\n",
    "    ):\n",
    "        self.semaphore = asyncio.Semaphore(\n",
    "            _adjust_concurrency_by_payment_status())\n",
    "        self.note_id = note_id\n",
    "        self.file_id = file_id\n",
    "        self.filename = filename\n",
    "        self.prompt_language = prompt_language\n",
    "        self.prompt_type = prompt_type\n",
    "        self.temperature = temperature\n",
    "        self.streaming = streaming\n",
    "        self.llm_callbacks = [AsyncIteratorCallbackHandler()]\n",
    "        self.question_count = 0\n",
    "\n",
    "    # 初始化模型参数和prompt\n",
    "    def _init_llm_chain(self, timeout: int = 10, role: str = \"\", question_type: str = \"\"):\n",
    "            # 初始化模型参数\n",
    "            llm_instance = LLM(\n",
    "                temperature=self.temperature,\n",
    "                streaming=self.streaming,\n",
    "                callbacks=self.llm_callbacks,\n",
    "                max_retries=_adjust_retries_by_payment_status(),\n",
    "                timeout=timeout\n",
    "            )\n",
    "            # 初始化prompt\n",
    "            prompt = choose_prompt(\n",
    "                role,\n",
    "                question_type,\n",
    "                self.prompt_language,\n",
    "                self.prompt_type\n",
    "            )\n",
    "\n",
    "            return LLMChain(\n",
    "                prompt=prompt,\n",
    "                llm=llm_instance.llm,\n",
    "            )\n",
    "\n",
    "\n",
    "    #########################################################################\n",
    "    #                       生成问题\n",
    "    #########################################################################\n",
    "\n",
    "    # input：笔记（拆分后）、笔记标题、问题类型（选择、填空）\n",
    "    async def agenerate_questions(\n",
    "        self,\n",
    "        # docs: list[Document],\n",
    "        docs, \n",
    "        title: str,\n",
    "        question_type: str\n",
    "    ):\n",
    "        tasks = []\n",
    "        # 生成llm chain\n",
    "        llm_chain = self._init_llm_chain(\n",
    "            timeout=60, question_type=question_type)\n",
    "        for doc in docs:\n",
    "            # 将分割好的文本写入数据库\n",
    "            doc_id = _dbs_.document.save_doc_to_db(\n",
    "                self.note_id, self.file_id, self.filename, doc.page_content)\n",
    "            # 调用_agenerate_questions，用分割好的文本生成问题\n",
    "            tasks.append(self._agenerate_questions(\n",
    "                llm_chain, doc, title, doc_id, question_type))\n",
    "\n",
    "        # 等待任务完成\n",
    "        try:\n",
    "            await asyncio.wait_for(asyncio.gather(*tasks), timeout=len(docs) * 60)\n",
    "        except Exception as e:\n",
    "            _dbs_.file.set_file_is_uploading_state(\n",
    "                self.file_id, self.question_count)\n",
    "            raise e\n",
    "        return self.question_count\n",
    "\n",
    "    async def _agenerate_questions(\n",
    "        self,\n",
    "        llm_chain: LLMChain,\n",
    "        doc: Document,\n",
    "        title: str,\n",
    "        doc_id: int,\n",
    "        question_type: str\n",
    "    ):\n",
    "        async with self.semaphore:\n",
    "            # 调用 LLM 链的 apredict 方法，生成基于文档内容和标题的文本。\n",
    "            res = await llm_chain.apredict(\n",
    "                title=title,\n",
    "                context=doc.page_content\n",
    "            )\n",
    "            # 将生成的文本分割成单个问题，并逐个检查其合法性。\n",
    "            # 如果问题合法，则将其保存到数据库，并更新问题的计数\n",
    "            for question in _spite_questions(res, question_type):\n",
    "                if not _is_legal_question_structure(question, question_type):\n",
    "                    continue\n",
    "                _dbs_.question.save_question_to_db(\n",
    "                    question_content=_remove_prefix_numbers(question),\n",
    "                    document_id=doc_id,\n",
    "                    question_type=question_type,\n",
    "                    designated_role=os.environ.get(\"CURRENT_ROLE\")\n",
    "                )\n",
    "                self.question_count += 1\n",
    "\n",
    "    #########################################################################\n",
    "    #                   检测用户回答的正确性\n",
    "    #########################################################################\n",
    "    async def aexamine_answer(\n",
    "        self,\n",
    "        quesiton_id: int,\n",
    "        context: str,\n",
    "        question: str,\n",
    "        answer: str,\n",
    "        role: str,\n",
    "        question_type: str,\n",
    "    ):\n",
    "        llm_chain = self._init_llm_chain(60, role, question_type)\n",
    "        coroutine = _wait_done(llm_chain.apredict(\n",
    "            context=context,\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            callbacks=self.llm_callbacks\n",
    "        ), self.llm_callbacks[0].done)\n",
    "\n",
    "        task = asyncio.create_task(coroutine)\n",
    "        examine = \"\"\n",
    "        async for token in self.llm_callbacks[0].aiter():\n",
    "            examine += token\n",
    "            yield f\"{token}\"\n",
    "\n",
    "        try:\n",
    "            await task\n",
    "        except Exception as e:\n",
    "            yield str(e)\n",
    "            return\n",
    "\n",
    "        score = _extract_score(examine)\n",
    "        push_date = _get_push_date(score)\n",
    "        await _dbs_.question.update_question_state(\n",
    "            id=quesiton_id,\n",
    "            answer=f\"{answer} ||| {examine}\",\n",
    "            score=score,\n",
    "            push_date=push_date\n",
    "        )\n",
    "\n",
    "\n",
    "# 等待任务完成\n",
    "async def _wait_done(\n",
    "    fn: Awaitable,\n",
    "    event: asyncio.Event\n",
    "):\n",
    "    try:\n",
    "        await fn\n",
    "    except Exception as e:\n",
    "        event.set()\n",
    "        raise e\n",
    "    finally:\n",
    "        event.set()\n",
    "\n",
    "\n",
    "################################################\n",
    "\n",
    "# 根据是否付费调整当前提问数\n",
    "def _adjust_concurrency_by_payment_status():\n",
    "    payment = os.environ.get(\"PAYMENT\", \"free\")\n",
    "    if (payment == \"free\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 3\n",
    "# 根据是否付费调整尝试数量\n",
    "def _adjust_retries_by_payment_status():\n",
    "    payment = os.environ.get(\"PAYMENT\", \"free\")\n",
    "    if (payment == \"free\"):\n",
    "        return 20\n",
    "    else:\n",
    "        return 6\n",
    "\n",
    "#################################### 生成问题 工具 #####################################\n",
    "\n",
    "# 拆分生成的题目\n",
    "def _spite_questions(\n",
    "    content: str,\n",
    "    type: str\n",
    "):\n",
    "    questions = []\n",
    "    if (type == \"choice\"):\n",
    "        questions = content.strip().split('\\n\\n')\n",
    "    else:\n",
    "        questions = content.strip().split(\"\\n\")\n",
    "    return questions\n",
    "\n",
    "# 检查question的格式是否正确\n",
    "def _is_legal_question_structure(\n",
    "    content: str,\n",
    "    type: str\n",
    "):\n",
    "    # 判断生成的问题是否合理\n",
    "    if content == \"\":\n",
    "        return False\n",
    "    if len(content) < 12:\n",
    "        return False\n",
    "\n",
    "    # 选择题格式改写（加横线、加选项）\n",
    "    if type == \"choice\":\n",
    "        pattern = r'^-\\s.+?\\n\\s*A\\..+\\n\\s*B\\..+\\n\\s*C\\..+\\n\\s*D\\..+$'\n",
    "        return bool(re.match(pattern, content))\n",
    "    if type == \"blank\":\n",
    "        return \"_____\" in content\n",
    "\n",
    "    return True\n",
    "\n",
    "# 移除题号\n",
    "def _remove_prefix_numbers(text):\n",
    "    cleaned_text = re.sub(r'^\\s*(?:\\d+\\.|-)\\s*', '', text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "#################################### 回复反馈 工具 #####################################\n",
    "\n",
    "# 从回答中得到分数\n",
    "def _extract_score(anwser: str):\n",
    "    score = re.findall(r\"\\d+\\.?\\d*\", anwser)\n",
    "    if score:\n",
    "        return int(float(score[0]))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# 根据艾宾浩斯遗忘曲线得到推送日期\n",
    "def _get_push_date(score: int):\n",
    "    now = datetime.datetime.now()\n",
    "    days = handle_ebbinghaus_memory(score)\n",
    "    return ((now+datetime.timedelta(days)).strftime(\"%Y-%m-%d\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADAPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
